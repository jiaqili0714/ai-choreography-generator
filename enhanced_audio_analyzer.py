"""
å¢å¼ºéŸ³é¢‘åˆ†æå™¨
é›†æˆlibrosaã€madmomã€Essentiaã€musicnnç­‰ä¸“ä¸šéŸ³é¢‘åˆ†æåº“
"""

import librosa
import numpy as np
import soundfile as sf
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

try:
    import madmom
    MADMOM_AVAILABLE = True
except ImportError:
    MADMOM_AVAILABLE = False
    print("Warning: madmom not available, using librosa beat tracking")

try:
    import essentia.standard as es
    ESSENTIA_AVAILABLE = True
except ImportError:
    ESSENTIA_AVAILABLE = False
    print("Warning: essentia not available, using basic features")

try:
    import musicnn
    MUSICNN_AVAILABLE = True
except ImportError:
    MUSICNN_AVAILABLE = False
    print("Warning: musicnn not available, using basic genre detection")

class EnhancedAudioAnalyzer:
    """å¢å¼ºéŸ³é¢‘åˆ†æå™¨"""
    
    def __init__(self):
        self.sample_rate = 22050
        self.hop_length = 512
        self.frame_length = 2048
        
    def load_audio(self, file_path: str) -> Tuple[np.ndarray, int]:
        """åŠ è½½éŸ³é¢‘æ–‡ä»¶"""
        try:
            y, sr = librosa.load(file_path, sr=self.sample_rate)
            return y, sr
        except Exception as e:
            raise Exception(f"Error loading audio: {e}")
    
    def extract_basic_features(self, y: np.ndarray, sr: int) -> Dict:
        """æå–åŸºç¡€éŸ³é¢‘ç‰¹å¾"""
        # BPMæ£€æµ‹
        tempo, beats = librosa.beat.beat_track(y=y, sr=sr, hop_length=self.hop_length)
        
        # é¢‘è°±ç‰¹å¾
        spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
        spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]
        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
        rms = librosa.feature.rms(y=y)[0]
        
        # èŠ‚æ‹ç‚¹
        beat_times = librosa.frames_to_time(beats, sr=sr, hop_length=self.hop_length)
        
        return {
            'tempo': float(tempo),
            'beats': beats,
            'beat_times': beat_times,
            'spectral_centroid_mean': float(np.mean(spectral_centroids)),
            'spectral_rolloff_mean': float(np.mean(spectral_rolloff)),
            'mfcc_mean': [float(x) for x in np.mean(mfccs, axis=1)],
            'rms_mean': float(np.mean(rms)),
            'duration': len(y) / sr
        }
    
    def extract_madmom_features(self, file_path: str) -> Dict:
        """ä½¿ç”¨madmomæå–ç²¾ç¡®èŠ‚æ‹ç‰¹å¾"""
        if not MADMOM_AVAILABLE:
            return {}
        
        try:
            # ä½¿ç”¨madmomçš„DBNDownBeatTrackingProcessor
            proc = madmom.features.downbeat.DBNDownBeatTrackingProcessor(beats_per_bar=[4])
            act = madmom.features.downbeat.RNNDownBeatProcessor()(file_path)
            downbeats = proc(act)
            
            # æå–èŠ‚æ‹ç‚¹
            beats = []
            for beat in downbeats:
                beats.append(beat[0])  # æ—¶é—´ç‚¹
            
            # è®¡ç®—BPM
            if len(beats) > 1:
                intervals = np.diff(beats)
                bpm = 60.0 / np.mean(intervals)
            else:
                bpm = 120.0
            
            return {
                'madmom_bpm': float(bpm),
                'madmom_beats': beats,
                'downbeats': downbeats.tolist()
            }
        except Exception as e:
            print(f"Madmom analysis failed: {e}")
            return {}
    
    def extract_essentia_features(self, file_path: str) -> Dict:
        """ä½¿ç”¨Essentiaæå–é«˜çº§éŸ³é¢‘ç‰¹å¾"""
        if not ESSENTIA_AVAILABLE:
            return {}
        
        try:
            # åŠ è½½éŸ³é¢‘
            loader = es.MonoLoader(filename=file_path, sampleRate=self.sample_rate)
            audio = loader()
            
            # æå–ç‰¹å¾
            rhythm_extractor = es.RhythmExtractor2013(method="multifeature")
            bpm, beats, beats_confidence, _, beats_intervals = rhythm_extractor(audio)
            
            # æƒ…ç»ªå’Œèƒ½é‡ç‰¹å¾
            mood_extractor = es.PredominantPitchMelodia()
            pitch, pitch_confidence = mood_extractor(audio)
            
            # é¢‘è°±ç‰¹å¾
            spectral_peaks = es.SpectralPeaks()
            frequencies, magnitudes = spectral_peaks(audio)
            
            # å’Œå£°ç‰¹å¾
            hpcp = es.HPCP()
            hpcp_values = hpcp(frequencies, magnitudes)
            
            return {
                'essentia_bpm': float(bpm),
                'essentia_beats': beats.tolist(),
                'beats_confidence': float(np.mean(beats_confidence)),
                'pitch_mean': float(np.mean(pitch)) if len(pitch) > 0 else 0,
                'pitch_confidence': float(np.mean(pitch_confidence)) if len(pitch_confidence) > 0 else 0,
                'hpcp_mean': [float(x) for x in np.mean(hpcp_values, axis=0)],
                'energy': float(np.mean(audio**2))
            }
        except Exception as e:
            print(f"Essentia analysis failed: {e}")
            return {}
    
    def extract_musicnn_features(self, file_path: str) -> Dict:
        """ä½¿ç”¨musicnnè¿›è¡ŒéŸ³ä¹é£æ ¼è¯†åˆ«"""
        if not MUSICNN_AVAILABLE:
            return self._basic_genre_detection()
        
        try:
            # ä½¿ç”¨musicnnè¿›è¡Œé£æ ¼è¯†åˆ«
            taggram, tags, features = musicnn.predict(file_path)
            
            # è·å–æœ€å¯èƒ½çš„é£æ ¼æ ‡ç­¾
            top_tags = tags[np.argsort(taggram.mean(axis=0))[-5:]]
            
            # æ˜ å°„åˆ°èˆè¹ˆé£æ ¼
            dance_style = self._map_genre_to_dance_style(top_tags)
            
            return {
                'musicnn_tags': top_tags.tolist(),
                'dance_style': dance_style,
                'style_confidence': float(np.max(taggram.mean(axis=0)))
            }
        except Exception as e:
            print(f"Musicnn analysis failed: {e}")
            return self._basic_genre_detection()
    
    def _basic_genre_detection(self) -> Dict:
        """åŸºç¡€é£æ ¼æ£€æµ‹ï¼ˆå½“musicnnä¸å¯ç”¨æ—¶ï¼‰"""
        return {
            'dance_style': 'Hip-Hop',
            'style_confidence': 0.5,
            'musicnn_tags': ['electronic', 'dance', 'hip-hop']
        }
    
    def _map_genre_to_dance_style(self, tags: List[str]) -> str:
        """å°†éŸ³ä¹é£æ ¼æ ‡ç­¾æ˜ å°„åˆ°èˆè¹ˆé£æ ¼"""
        tag_str = ' '.join(tags).lower()
        
        if any(word in tag_str for word in ['hip', 'hop', 'rap', 'urban']):
            return 'Hip-Hop'
        elif any(word in tag_str for word in ['house', 'electronic', 'techno', 'edm']):
            return 'House'
        elif any(word in tag_str for word in ['k-pop', 'pop', 'korean']):
            return 'K-pop'
        elif any(word in tag_str for word in ['jazz', 'blues', 'swing']):
            return 'Jazz'
        elif any(word in tag_str for word in ['contemporary', 'modern', 'ballet']):
            return 'Contemporary'
        elif any(word in tag_str for word in ['break', 'breaking', 'b-boy']):
            return 'Breaking'
        else:
            return 'Hip-Hop'  # é»˜è®¤é£æ ¼
    
    def segment_audio_by_beats(self, beat_times: np.ndarray, beats_per_segment: int = 8) -> List[Dict]:
        """æŒ‰èŠ‚æ‹åˆ†å‰²éŸ³é¢‘"""
        segments = []
        
        for i in range(0, len(beat_times) - beats_per_segment + 1, beats_per_segment):
            start_time = beat_times[i]
            end_time = beat_times[i + beats_per_segment - 1] if i + beats_per_segment < len(beat_times) else beat_times[-1]
            
            segments.append({
                'segment_id': len(segments),
                'start_time': float(start_time),
                'end_time': float(end_time),
                'duration': float(end_time - start_time),
                'start_beat': i,
                'end_beat': i + beats_per_segment - 1
            })
        
        return segments
    
    def analyze_audio_segment(self, y: np.ndarray, sr: int, start_time: float, end_time: float) -> Dict:
        """åˆ†æéŸ³é¢‘ç‰‡æ®µç‰¹å¾"""
        start_sample = int(start_time * sr)
        end_sample = int(end_time * sr)
        segment = y[start_sample:end_sample]
        
        if len(segment) == 0:
            return {}
        
        # æå–ç‰‡æ®µç‰¹å¾
        tempo, _ = librosa.beat.beat_track(y=segment, sr=sr)
        spectral_centroid = librosa.feature.spectral_centroid(y=segment, sr=sr)[0]
        rms = librosa.feature.rms(y=segment)[0]
        
        return {
            'tempo': float(tempo),
            'energy': float(np.mean(rms)),
            'brightness': float(np.mean(spectral_centroid)),
            'complexity': float(np.std(spectral_centroid))
        }
    
    def comprehensive_analysis(self, file_path: str) -> Dict:
        """ç»¼åˆåˆ†æéŸ³é¢‘æ–‡ä»¶"""
        print("ğŸµ å¼€å§‹ç»¼åˆåˆ†æéŸ³é¢‘...")
        
        # åŠ è½½éŸ³é¢‘
        y, sr = self.load_audio(file_path)
        
        # åŸºç¡€ç‰¹å¾
        print("ğŸ“Š æå–åŸºç¡€ç‰¹å¾...")
        basic_features = self.extract_basic_features(y, sr)
        
        # Madmomç‰¹å¾
        print("ğŸ¯ æå–ç²¾ç¡®èŠ‚æ‹ç‰¹å¾...")
        madmom_features = self.extract_madmom_features(file_path)
        
        # Essentiaç‰¹å¾
        print("ğŸ¨ æå–é«˜çº§éŸ³é¢‘ç‰¹å¾...")
        essentia_features = self.extract_essentia_features(file_path)
        
        # Musicnnç‰¹å¾
        print("ğŸ­ è¯†åˆ«éŸ³ä¹é£æ ¼...")
        musicnn_features = self.extract_musicnn_features(file_path)
        
        # é€‰æ‹©æœ€ä½³BPM
        bpm_candidates = [basic_features['tempo']]
        if 'madmom_bpm' in madmom_features:
            bpm_candidates.append(madmom_features['madmom_bpm'])
        if 'essentia_bpm' in essentia_features:
            bpm_candidates.append(essentia_features['essentia_bpm'])
        
        final_bpm = np.median(bpm_candidates)
        
        # é€‰æ‹©æœ€ä½³èŠ‚æ‹ç‚¹
        if 'madmom_beats' in madmom_features and len(madmom_features['madmom_beats']) > 0:
            beat_times = np.array(madmom_features['madmom_beats'])
        else:
            beat_times = basic_features['beat_times']
        
        # åˆ†å‰²éŸ³é¢‘
        segments = self.segment_audio_by_beats(beat_times)
        
        # åˆ†ææ¯ä¸ªç‰‡æ®µ
        segment_features = []
        for segment in segments:
            seg_features = self.analyze_audio_segment(
                y, sr, segment['start_time'], segment['end_time']
            )
            segment_features.append({**segment, **seg_features})
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        result = {
            'audio_info': {
                'duration': basic_features['duration'],
                'bpm': final_bpm,
                'total_beats': len(beat_times),
                'sample_rate': sr
            },
            'features': {
                **basic_features,
                **madmom_features,
                **essentia_features,
                **musicnn_features
            },
            'segments': segment_features,
            'dance_style': musicnn_features.get('dance_style', 'Hip-Hop'),
            'style_confidence': musicnn_features.get('style_confidence', 0.5)
        }
        
        print(f"âœ… åˆ†æå®Œæˆï¼æ£€æµ‹åˆ° {len(segments)} ä¸ª8æ‹ç‰‡æ®µï¼ŒBPM: {final_bpm:.1f}")
        print(f"ğŸ­ æ¨èèˆè¹ˆé£æ ¼: {result['dance_style']}")
        
        return result
