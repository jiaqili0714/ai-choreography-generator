"""
Streamlit Cloudå…¼å®¹çš„éŸ³é¢‘åˆ†æå™¨
ä½¿ç”¨åŸºç¡€åº“å®ç°ä¸“ä¸šçº§éŸ³é¢‘åˆ†æåŠŸèƒ½
"""

import librosa
import numpy as np
import soundfile as sf
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

class StreamlitCloudAudioAnalyzer:
    """Streamlit Cloudå…¼å®¹çš„éŸ³é¢‘åˆ†æå™¨"""
    
    def __init__(self):
        self.sample_rate = 22050
        self.hop_length = 512
        self.frame_length = 2048
        
    def load_audio(self, file_path: str) -> Tuple[np.ndarray, int]:
        """åŠ è½½éŸ³é¢‘æ–‡ä»¶"""
        try:
            y, sr = librosa.load(file_path, sr=self.sample_rate)
            return y, sr
        except Exception as e:
            raise Exception(f"Error loading audio: {e}")
    
    def extract_enhanced_features(self, y: np.ndarray, sr: int) -> Dict:
        """æå–å¢å¼ºéŸ³é¢‘ç‰¹å¾"""
        # BPMæ£€æµ‹
        tempo, beats = librosa.beat.beat_track(y=y, sr=sr, hop_length=self.hop_length)
        
        # é¢‘è°±ç‰¹å¾
        spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
        spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]
        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]
        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
        rms = librosa.feature.rms(y=y)[0]
        
        # èŠ‚æ‹ç‚¹
        beat_times = librosa.frames_to_time(beats, sr=sr, hop_length=self.hop_length)
        
        # é«˜çº§ç‰¹å¾è®¡ç®—
        zero_crossing_rate = librosa.feature.zero_crossing_rate(y)[0]
        chroma = librosa.feature.chroma_stft(y=y, sr=sr)
        tonnetz = librosa.feature.tonnetz(y=y, sr=sr)
        
        # èƒ½é‡å’ŒåŠ¨æ€ç‰¹å¾
        energy = np.mean(rms)
        energy_std = np.std(rms)
        spectral_centroid_mean = np.mean(spectral_centroids)
        spectral_centroid_std = np.std(spectral_centroids)
        
        # èŠ‚å¥ç‰¹å¾
        tempo_confidence = self._calculate_tempo_confidence(beats, sr)
        
        # æƒ…ç»ªå’Œé£æ ¼ç‰¹å¾
        mood_features = self._extract_mood_features(y, sr)
        style_features = self._extract_style_features(y, sr)
        
        return {
            'tempo': float(tempo),
            'tempo_confidence': float(tempo_confidence),
            'beats': beats,
            'beat_times': beat_times,
            'energy': float(energy),
            'energy_std': float(energy_std),
            'spectral_centroid_mean': float(spectral_centroid_mean),
            'spectral_centroid_std': float(spectral_centroid_std),
            'spectral_rolloff_mean': float(np.mean(spectral_rolloff)),
            'spectral_bandwidth_mean': float(np.mean(spectral_bandwidth)),
            'mfcc_mean': [float(x) for x in np.mean(mfccs, axis=1)],
            'zero_crossing_rate_mean': float(np.mean(zero_crossing_rate)),
            'chroma_mean': [float(x) for x in np.mean(chroma, axis=1)],
            'tonnetz_mean': [float(x) for x in np.mean(tonnetz, axis=1)],
            'duration': len(y) / sr,
            'mood_features': mood_features,
            'style_features': style_features
        }
    
    def _calculate_tempo_confidence(self, beats: np.ndarray, sr: int) -> float:
        """è®¡ç®—èŠ‚æ‹æ£€æµ‹ç½®ä¿¡åº¦"""
        if len(beats) < 2:
            return 0.0
        
        # è®¡ç®—èŠ‚æ‹é—´éš”çš„ä¸€è‡´æ€§
        intervals = np.diff(beats)
        if len(intervals) == 0:
            return 0.0
        
        # ä½¿ç”¨å˜å¼‚ç³»æ•°ä½œä¸ºç½®ä¿¡åº¦æŒ‡æ ‡
        cv = np.std(intervals) / np.mean(intervals) if np.mean(intervals) > 0 else 1.0
        confidence = max(0.0, 1.0 - cv)
        return confidence
    
    def _extract_mood_features(self, y: np.ndarray, sr: int) -> Dict:
        """æå–æƒ…ç»ªç‰¹å¾"""
        # ä½¿ç”¨é¢‘è°±ç‰¹å¾æ¨æ–­æƒ…ç»ª
        spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
        rms = librosa.feature.rms(y=y)[0]
        
        # è®¡ç®—æƒ…ç»ªæŒ‡æ ‡
        brightness = np.mean(spectral_centroids)
        energy = np.mean(rms)
        energy_variation = np.std(rms)
        
        # æƒ…ç»ªåˆ†ç±»
        if brightness > np.percentile(spectral_centroids, 75) and energy > np.percentile(rms, 75):
            mood = "energetic"
            valence = 0.8
        elif brightness < np.percentile(spectral_centroids, 25) and energy < np.percentile(rms, 25):
            mood = "calm"
            valence = 0.3
        elif energy_variation > np.percentile(rms, 75):
            mood = "dynamic"
            valence = 0.6
        else:
            mood = "neutral"
            valence = 0.5
        
        return {
            'mood': mood,
            'valence': float(valence),
            'brightness': float(brightness),
            'energy': float(energy),
            'energy_variation': float(energy_variation)
        }
    
    def _extract_style_features(self, y: np.ndarray, sr: int) -> Dict:
        """æå–é£æ ¼ç‰¹å¾"""
        # ä½¿ç”¨MFCCå’Œé¢‘è°±ç‰¹å¾æ¨æ–­é£æ ¼
        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
        spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
        rms = librosa.feature.rms(y=y)[0]
        
        # è®¡ç®—é£æ ¼æŒ‡æ ‡
        mfcc_mean = np.mean(mfccs, axis=1)
        spectral_centroid_mean = np.mean(spectral_centroids)
        energy_mean = np.mean(rms)
        
        # é£æ ¼åˆ†ç±»é€»è¾‘
        if energy_mean > 0.1 and spectral_centroid_mean > 2000:
            if mfcc_mean[1] > 0:  # é«˜é¢‘æˆåˆ†å¤š
                style = "Hip-Hop"
                confidence = 0.8
            else:
                style = "House"
                confidence = 0.7
        elif energy_mean < 0.05 and spectral_centroid_mean < 1500:
            style = "Jazz"
            confidence = 0.6
        elif mfcc_mean[0] < -5:  # ä½é¢‘æˆåˆ†å¤š
            style = "Breaking"
            confidence = 0.7
        else:
            style = "K-pop"
            confidence = 0.5
        
        return {
            'dance_style': style,
            'style_confidence': float(confidence),
            'mfcc_features': [float(x) for x in mfcc_mean],
            'spectral_features': {
                'centroid': float(spectral_centroid_mean),
                'energy': float(energy_mean)
            }
        }
    
    def segment_audio_by_beats(self, beat_times: np.ndarray, beats_per_segment: int = 8) -> List[Dict]:
        """æŒ‰èŠ‚æ‹åˆ†å‰²éŸ³é¢‘"""
        segments = []
        
        for i in range(0, len(beat_times) - beats_per_segment + 1, beats_per_segment):
            start_time = beat_times[i]
            end_time = beat_times[i + beats_per_segment - 1] if i + beats_per_segment < len(beat_times) else beat_times[-1]
            
            segments.append({
                'segment_id': len(segments),
                'start_time': float(start_time),
                'end_time': float(end_time),
                'duration': float(end_time - start_time),
                'start_beat': i,
                'end_beat': i + beats_per_segment - 1
            })
        
        return segments
    
    def analyze_audio_segment(self, y: np.ndarray, sr: int, start_time: float, end_time: float) -> Dict:
        """åˆ†æéŸ³é¢‘ç‰‡æ®µç‰¹å¾"""
        start_sample = int(start_time * sr)
        end_sample = int(end_time * sr)
        segment = y[start_sample:end_sample]
        
        if len(segment) == 0:
            return {}
        
        # æå–ç‰‡æ®µç‰¹å¾
        tempo, _ = librosa.beat.beat_track(y=segment, sr=sr)
        spectral_centroid = librosa.feature.spectral_centroid(y=segment, sr=sr)[0]
        rms = librosa.feature.rms(y=segment)[0]
        mfccs = librosa.feature.mfcc(y=segment, sr=sr, n_mfcc=13)
        
        # è®¡ç®—å¤æ‚åº¦
        complexity = np.std(spectral_centroid) / np.mean(spectral_centroid) if np.mean(spectral_centroid) > 0 else 0
        
        return {
            'tempo': float(tempo),
            'energy': float(np.mean(rms)),
            'brightness': float(np.mean(spectral_centroid)),
            'complexity': float(complexity),
            'mfcc_mean': [float(x) for x in np.mean(mfccs, axis=1)]
        }
    
    def comprehensive_analysis(self, file_path: str) -> Dict:
        """ç»¼åˆåˆ†æéŸ³é¢‘æ–‡ä»¶"""
        print("ğŸµ å¼€å§‹ç»¼åˆåˆ†æéŸ³é¢‘...")
        
        # åŠ è½½éŸ³é¢‘
        y, sr = self.load_audio(file_path)
        
        # æå–å¢å¼ºç‰¹å¾
        print("ğŸ“Š æå–å¢å¼ºç‰¹å¾...")
        features = self.extract_enhanced_features(y, sr)
        
        # åˆ†å‰²éŸ³é¢‘
        segments = self.segment_audio_by_beats(features['beat_times'])
        
        # åˆ†ææ¯ä¸ªç‰‡æ®µ
        segment_features = []
        for segment in segments:
            seg_features = self.analyze_audio_segment(
                y, sr, segment['start_time'], segment['end_time']
            )
            segment_features.append({**segment, **seg_features})
        
        # æ„å»ºç»“æœ
        result = {
            'audio_info': {
                'duration': float(features['duration']),
                'bpm': float(features['tempo']),
                'total_beats': len(features['beat_times']),
                'sample_rate': sr
            },
            'features': features,
            'segments': segment_features,
            'dance_style': features['style_features']['dance_style'],
            'style_confidence': features['style_features']['style_confidence']
        }
        
        print(f"âœ… åˆ†æå®Œæˆï¼æ£€æµ‹åˆ° {len(segments)} ä¸ª8æ‹ç‰‡æ®µï¼ŒBPM: {features['tempo']:.1f}")
        print(f"ğŸ­ æ¨èèˆè¹ˆé£æ ¼: {result['dance_style']}")
        
        return result
