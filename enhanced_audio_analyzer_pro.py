"""
ä¸“ä¸šçº§éŸ³é¢‘åˆ†æå™¨
é›†æˆlibrosaã€madmomã€essentiaã€musicnnç­‰é«˜çº§éŸ³é¢‘åˆ†æåº“
"""

import librosa
import numpy as np
import soundfile as sf
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# å°è¯•å¯¼å…¥é«˜çº§éŸ³é¢‘åˆ†æåº“
try:
    import madmom
    MADMOM_AVAILABLE = True
    print("âœ… madmom å·²åŠ è½½")
except ImportError:
    MADMOM_AVAILABLE = False
    print("âš ï¸ madmom ä¸å¯ç”¨ï¼Œä½¿ç”¨librosaæ›¿ä»£")

try:
    import essentia.standard as es
    ESSENTIA_AVAILABLE = True
    print("âœ… essentia å·²åŠ è½½")
except ImportError:
    ESSENTIA_AVAILABLE = False
    print("âš ï¸ essentia ä¸å¯ç”¨ï¼Œä½¿ç”¨librosaæ›¿ä»£")

try:
    import musicnn
    MUSICNN_AVAILABLE = True
    print("âœ… musicnn å·²åŠ è½½")
except ImportError:
    MUSICNN_AVAILABLE = False
    print("âš ï¸ musicnn ä¸å¯ç”¨ï¼Œä½¿ç”¨ç‰¹å¾å·¥ç¨‹æ›¿ä»£")

class EnhancedAudioAnalyzerPro:
    """ä¸“ä¸šçº§éŸ³é¢‘åˆ†æå™¨"""
    
    def __init__(self):
        self.sample_rate = 22050
        self.hop_length = 512
        self.frame_length = 2048
        
    def load_audio(self, file_path: str) -> Tuple[np.ndarray, int]:
        """åŠ è½½éŸ³é¢‘æ–‡ä»¶"""
        try:
            y, sr = librosa.load(file_path, sr=self.sample_rate)
            return y, sr
        except Exception as e:
            raise Exception(f"Error loading audio: {e}")
    
    def extract_enhanced_features(self, y: np.ndarray, sr: int) -> Dict:
        """æå–å¢å¼ºéŸ³é¢‘ç‰¹å¾"""
        print("ğŸµ å¼€å§‹ä¸“ä¸šçº§éŸ³é¢‘åˆ†æ...")
        
        # åŸºç¡€ç‰¹å¾ (librosa)
        tempo, beats = librosa.beat.beat_track(y=y, sr=sr, hop_length=self.hop_length)
        spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
        spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]
        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]
        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
        rms = librosa.feature.rms(y=y)[0]
        zero_crossing_rate = librosa.feature.zero_crossing_rate(y)[0]
        chroma = librosa.feature.chroma_stft(y=y, sr=sr)
        tonnetz = librosa.feature.tonnetz(y=y, sr=sr)
        
        # é«˜çº§èŠ‚æ‹æ£€æµ‹ (madmom)
        if MADMOM_AVAILABLE:
            print("ğŸ¯ ä½¿ç”¨madmomè¿›è¡Œç²¾ç¡®èŠ‚æ‹æ£€æµ‹...")
            try:
                # ä½¿ç”¨madmomçš„DBNDownBeatTrackingProcessor
                proc = madmom.features.downbeats.DBNDownBeatTrackingProcessor(
                    beats_per_bar=[3, 4], fps=100)
                act = madmom.features.downbeats.RNNDownBeatProcessor()(file_path)
                downbeats = proc(act)
                
                # æå–èŠ‚æ‹ä¿¡æ¯
                beat_times = downbeats[:, 0]
                beat_labels = downbeats[:, 1]
                
                # è®¡ç®—èŠ‚æ‹ç½®ä¿¡åº¦
                tempo_confidence = self._calculate_madmom_confidence(beat_times, sr)
                
                print(f"âœ… madmomæ£€æµ‹åˆ° {len(beat_times)} ä¸ªèŠ‚æ‹ç‚¹")
                
            except Exception as e:
                print(f"âš ï¸ madmomå¤„ç†å¤±è´¥: {e}")
                beat_times = librosa.frames_to_time(beats, sr=sr, hop_length=self.hop_length)
                beat_labels = np.ones(len(beat_times))
                tempo_confidence = 0.7
        else:
            beat_times = librosa.frames_to_time(beats, sr=sr, hop_length=self.hop_length)
            beat_labels = np.ones(len(beat_times))
            tempo_confidence = 0.7
        
        # é«˜çº§éŸ³é¢‘ç‰¹å¾ (essentia)
        if ESSENTIA_AVAILABLE:
            print("ğŸ¨ ä½¿ç”¨essentiaæå–é«˜çº§ç‰¹å¾...")
            try:
                # åŠ è½½éŸ³é¢‘åˆ°essentia
                loader = es.MonoLoader(filename=file_path, sampleRate=sr)
                audio = loader()
                
                # æå–é«˜çº§ç‰¹å¾
                rhythm_extractor = es.RhythmExtractor2013(method="multifeature")
                bpm, beats, beats_confidence, _, beats_intervals = rhythm_extractor(audio)
                
                # æƒ…ç»ªå’Œé£æ ¼ç‰¹å¾
                mood_extractor = es.PredominantPitchMelodia()
                pitch, pitch_confidence = mood_extractor(audio)
                
                # é¢‘è°±ç‰¹å¾
                spectral_peaks = es.SpectralPeaks()
                frequencies, magnitudes = spectral_peaks(audio)
                
                # å’Œå£°ç‰¹å¾
                hpcp = es.HPCP()
                hpcp_values = hpcp(frequencies, magnitudes)
                
                # æ„å»ºessentiaç‰¹å¾
                essentia_features = {
                    'bpm_essentia': float(bpm),
                    'beats_confidence': float(beats_confidence),
                    'pitch_mean': float(np.mean(pitch)),
                    'pitch_std': float(np.std(pitch)),
                    'hpcp_mean': [float(x) for x in np.mean(hpcp_values, axis=0)],
                    'spectral_peaks_count': len(frequencies)
                }
                
                print("âœ… essentiaç‰¹å¾æå–å®Œæˆ")
                
            except Exception as e:
                print(f"âš ï¸ essentiaå¤„ç†å¤±è´¥: {e}")
                essentia_features = {}
        else:
            essentia_features = {}
        
        # éŸ³ä¹é£æ ¼è¯†åˆ« (musicnn)
        if MUSICNN_AVAILABLE:
            print("ğŸ­ ä½¿ç”¨musicnnè¿›è¡Œé£æ ¼è¯†åˆ«...")
            try:
                # ä½¿ç”¨musicnnè¿›è¡Œé£æ ¼è¯†åˆ«
                taggram, tags, features = musicnn.extract(file_path, model='MSD_musicnn', extract_features=True)
                
                # è·å–æœ€å¯èƒ½çš„é£æ ¼æ ‡ç­¾
                top_tags = tags[np.argsort(taggram.mean(axis=0))[-5:]]
                style_confidence = float(np.max(taggram.mean(axis=0)))
                
                # æ„å»ºé£æ ¼ç‰¹å¾
                musicnn_features = {
                    'top_tags': [str(tag) for tag in top_tags],
                    'style_confidence': style_confidence,
                    'taggram_mean': [float(x) for x in taggram.mean(axis=0)]
                }
                
                print(f"âœ… musicnnè¯†åˆ«é£æ ¼: {top_tags}")
                
            except Exception as e:
                print(f"âš ï¸ musicnnå¤„ç†å¤±è´¥: {e}")
                musicnn_features = {}
        else:
            musicnn_features = {}
        
        # è®¡ç®—åŸºç¡€ç‰¹å¾
        energy = np.mean(rms)
        energy_std = np.std(rms)
        spectral_centroid_mean = np.mean(spectral_centroids)
        spectral_centroid_std = np.std(spectral_centroids)
        
        # æƒ…ç»ªå’Œé£æ ¼ç‰¹å¾
        mood_features = self._extract_mood_features_enhanced(y, sr, essentia_features)
        style_features = self._extract_style_features_enhanced(y, sr, musicnn_features)
        
        # æ„å»ºç»“æœ
        result = {
            'tempo': float(tempo),
            'tempo_confidence': float(tempo_confidence),
            'beats': beats,
            'beat_times': beat_times,
            'beat_labels': beat_labels,
            'energy': float(energy),
            'energy_std': float(energy_std),
            'spectral_centroid_mean': float(spectral_centroid_mean),
            'spectral_centroid_std': float(spectral_centroid_std),
            'spectral_rolloff_mean': float(np.mean(spectral_rolloff)),
            'spectral_bandwidth_mean': float(np.mean(spectral_bandwidth)),
            'mfcc_mean': [float(x) for x in np.mean(mfccs, axis=1)],
            'zero_crossing_rate_mean': float(np.mean(zero_crossing_rate)),
            'chroma_mean': [float(x) for x in np.mean(chroma, axis=1)],
            'tonnetz_mean': [float(x) for x in np.mean(tonnetz, axis=1)],
            'duration': len(y) / sr,
            'mood_features': mood_features,
            'style_features': style_features,
            'essentia_features': essentia_features,
            'musicnn_features': musicnn_features,
            'madmom_available': MADMOM_AVAILABLE,
            'essentia_available': ESSENTIA_AVAILABLE,
            'musicnn_available': MUSICNN_AVAILABLE
        }
        
        print(f"âœ… ä¸“ä¸šçº§åˆ†æå®Œæˆï¼BPM: {tempo:.1f}, ç½®ä¿¡åº¦: {tempo_confidence:.2f}")
        return result
    
    def _calculate_madmom_confidence(self, beat_times: np.ndarray, sr: int) -> float:
        """è®¡ç®—madmomèŠ‚æ‹æ£€æµ‹ç½®ä¿¡åº¦"""
        if len(beat_times) < 2:
            return 0.0
        
        intervals = np.diff(beat_times)
        if len(intervals) == 0:
            return 0.0
        
        # ä½¿ç”¨å˜å¼‚ç³»æ•°ä½œä¸ºç½®ä¿¡åº¦æŒ‡æ ‡
        cv = np.std(intervals) / np.mean(intervals) if np.mean(intervals) > 0 else 1.0
        confidence = max(0.0, 1.0 - cv)
        return confidence
    
    def _extract_mood_features_enhanced(self, y: np.ndarray, sr: int, essentia_features: Dict) -> Dict:
        """æå–å¢å¼ºæƒ…ç»ªç‰¹å¾"""
        spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
        rms = librosa.feature.rms(y=y)[0]
        
        # åŸºç¡€æƒ…ç»ªæŒ‡æ ‡
        brightness = np.mean(spectral_centroids)
        energy = np.mean(rms)
        energy_variation = np.std(rms)
        
        # å¦‚æœæœ‰essentiaç‰¹å¾ï¼Œä½¿ç”¨æ›´ç²¾ç¡®çš„æƒ…ç»ªåˆ†æ
        if essentia_features:
            pitch_mean = essentia_features.get('pitch_mean', brightness)
            pitch_std = essentia_features.get('pitch_std', energy_variation)
            
            # æ›´ç²¾ç¡®çš„æƒ…ç»ªåˆ†ç±»
            if pitch_mean > np.percentile(spectral_centroids, 80) and energy > np.percentile(rms, 80):
                mood = "energetic"
                valence = 0.9
            elif pitch_mean < np.percentile(spectral_centroids, 20) and energy < np.percentile(rms, 20):
                mood = "calm"
                valence = 0.2
            elif pitch_std > np.percentile(rms, 80):
                mood = "dynamic"
                valence = 0.7
            else:
                mood = "neutral"
                valence = 0.5
        else:
            # åŸºç¡€æƒ…ç»ªåˆ†ç±»
            if brightness > np.percentile(spectral_centroids, 75) and energy > np.percentile(rms, 75):
                mood = "energetic"
                valence = 0.8
            elif brightness < np.percentile(spectral_centroids, 25) and energy < np.percentile(rms, 25):
                mood = "calm"
                valence = 0.3
            else:
                mood = "neutral"
                valence = 0.5
        
        return {
            'mood': mood,
            'valence': float(valence),
            'brightness': float(brightness),
            'energy': float(energy),
            'energy_variation': float(energy_variation)
        }
    
    def _extract_style_features_enhanced(self, y: np.ndarray, sr: int, musicnn_features: Dict) -> Dict:
        """æå–å¢å¼ºé£æ ¼ç‰¹å¾"""
        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
        spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
        rms = librosa.feature.rms(y=y)[0]
        
        # å¦‚æœæœ‰musicnnç‰¹å¾ï¼Œä½¿ç”¨æ›´ç²¾ç¡®çš„é£æ ¼è¯†åˆ«
        if musicnn_features:
            top_tags = musicnn_features.get('top_tags', [])
            style_confidence = musicnn_features.get('style_confidence', 0.5)
            
            # æ ¹æ®musicnnæ ‡ç­¾ç¡®å®šèˆè¹ˆé£æ ¼
            if any(tag in ['hip hop', 'rap', 'urban'] for tag in top_tags):
                style = "Hip-Hop"
                confidence = style_confidence
            elif any(tag in ['house', 'electronic', 'dance'] for tag in top_tags):
                style = "House"
                confidence = style_confidence
            elif any(tag in ['jazz', 'blues', 'soul'] for tag in top_tags):
                style = "Jazz"
                confidence = style_confidence
            elif any(tag in ['pop', 'k-pop', 'korean'] for tag in top_tags):
                style = "K-pop"
                confidence = style_confidence
            else:
                style = "Contemporary"
                confidence = style_confidence * 0.8
        else:
            # åŸºç¡€é£æ ¼åˆ†ç±»
            mfcc_mean = np.mean(mfccs, axis=1)
            spectral_centroid_mean = np.mean(spectral_centroids)
            energy_mean = np.mean(rms)
            
            if energy_mean > 0.1 and spectral_centroid_mean > 2000:
                if mfcc_mean[1] > 0:
                    style = "Hip-Hop"
                    confidence = 0.8
                else:
                    style = "House"
                    confidence = 0.7
            elif energy_mean < 0.05 and spectral_centroid_mean < 1500:
                style = "Jazz"
                confidence = 0.6
            else:
                style = "K-pop"
                confidence = 0.5
        
        return {
            'dance_style': style,
            'style_confidence': float(confidence),
            'mfcc_features': [float(x) for x in np.mean(mfccs, axis=1)],
            'spectral_features': {
                'centroid': float(spectral_centroid_mean),
                'energy': float(energy_mean)
            }
        }
    
    def segment_audio_by_beats(self, beat_times: np.ndarray, beats_per_segment: int = 8) -> List[Dict]:
        """æŒ‰èŠ‚æ‹åˆ†å‰²éŸ³é¢‘"""
        segments = []
        
        for i in range(0, len(beat_times) - beats_per_segment + 1, beats_per_segment):
            start_time = beat_times[i]
            end_time = beat_times[i + beats_per_segment - 1] if i + beats_per_segment < len(beat_times) else beat_times[-1]
            
            segments.append({
                'segment_id': len(segments),
                'start_time': float(start_time),
                'end_time': float(end_time),
                'duration': float(end_time - start_time),
                'start_beat': i,
                'end_beat': i + beats_per_segment - 1
            })
        
        return segments
    
    def analyze_audio_segment(self, y: np.ndarray, sr: int, start_time: float, end_time: float) -> Dict:
        """åˆ†æéŸ³é¢‘ç‰‡æ®µç‰¹å¾"""
        start_sample = int(start_time * sr)
        end_sample = int(end_time * sr)
        segment = y[start_sample:end_sample]
        
        if len(segment) == 0:
            return {}
        
        # æå–ç‰‡æ®µç‰¹å¾
        tempo, _ = librosa.beat.beat_track(y=segment, sr=sr)
        spectral_centroid = librosa.feature.spectral_centroid(y=segment, sr=sr)[0]
        rms = librosa.feature.rms(y=segment)[0]
        mfccs = librosa.feature.mfcc(y=segment, sr=sr, n_mfcc=13)
        
        # è®¡ç®—å¤æ‚åº¦
        complexity = np.std(spectral_centroid) / np.mean(spectral_centroid) if np.mean(spectral_centroid) > 0 else 0
        
        return {
            'tempo': float(tempo),
            'energy': float(np.mean(rms)),
            'brightness': float(np.mean(spectral_centroid)),
            'complexity': float(complexity),
            'mfcc_mean': [float(x) for x in np.mean(mfccs, axis=1)]
        }
    
    def comprehensive_analysis(self, file_path: str) -> Dict:
        """ç»¼åˆåˆ†æéŸ³é¢‘æ–‡ä»¶"""
        print("ğŸµ å¼€å§‹ä¸“ä¸šçº§ç»¼åˆåˆ†æ...")
        
        # åŠ è½½éŸ³é¢‘
        y, sr = self.load_audio(file_path)
        
        # æå–å¢å¼ºç‰¹å¾
        print("ğŸ“Š æå–ä¸“ä¸šçº§ç‰¹å¾...")
        features = self.extract_enhanced_features(y, sr)
        
        # åˆ†å‰²éŸ³é¢‘
        segments = self.segment_audio_by_beats(features['beat_times'])
        
        # åˆ†ææ¯ä¸ªç‰‡æ®µ
        segment_features = []
        for segment in segments:
            seg_features = self.analyze_audio_segment(
                y, sr, segment['start_time'], segment['end_time']
            )
            segment_features.append({**segment, **seg_features})
        
        # æ„å»ºç»“æœ
        result = {
            'audio_info': {
                'duration': features['duration'],
                'bpm': features['tempo'],
                'total_beats': len(features['beat_times']),
                'sample_rate': sr
            },
            'features': features,
            'segments': segment_features,
            'dance_style': features['style_features']['dance_style'],
            'style_confidence': features['style_features']['style_confidence'],
            'analysis_method': 'professional_enhanced'
        }
        
        print(f"âœ… ä¸“ä¸šçº§åˆ†æå®Œæˆï¼æ£€æµ‹åˆ° {len(segments)} ä¸ª8æ‹ç‰‡æ®µï¼ŒBPM: {features['tempo']:.1f}")
        print(f"ğŸ­ æ¨èèˆè¹ˆé£æ ¼: {result['dance_style']}")
        print(f"ğŸ”§ ä½¿ç”¨çš„é«˜çº§åº“: madmom={MADMOM_AVAILABLE}, essentia={ESSENTIA_AVAILABLE}, musicnn={MUSICNN_AVAILABLE}")
        
        return result
